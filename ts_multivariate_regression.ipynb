{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6DjYpXSuscFT",
        "uzaZI_1Je0_u"
      ],
      "authorship_tag": "ABX9TyOReXoxDGVyEgZKhkKxPg6J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewginns/Timeseries-Multivariate-Regression/blob/main/ts_multivariate_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Timeseries data preparation from (rare) event data\n",
        "\n",
        "The aim if to create a generic set of classes/functions that are able to take a\n",
        "timeseries dataset that can be used to predict a single output.\n",
        "\n",
        "The use case is highly similar to 'direct' timeseries prediction."
      ],
      "metadata": {
        "id": "ZW1c1LcijQfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pQBKQSzAl7ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Toy Dataset"
      ],
      "metadata": {
        "id": "cW_aGbNDMyYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://archive.ics.uci.edu/static/public/264/eeg+eye+state.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bUV50cHlwjW",
        "outputId": "72fd007f-9014-478e-f4bb-673cf180a254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-18 17:11:14--  http://archive.ics.uci.edu/static/public/264/eeg+eye+state.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘eeg+eye+state.zip.1’\n",
            "\n",
            "eeg+eye+state.zip.1     [    <=>             ]   1.62M  1.79MB/s    in 0.9s    \n",
            "\n",
            "2023-06-18 17:11:16 (1.79 MB/s) - ‘eeg+eye+state.zip.1’ saved [1696562]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o /content/eeg+eye+state.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl77VZ9JmOsZ",
        "outputId": "9f2a5158-130f-4a60-9b7e-109be7449fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/eeg+eye+state.zip\n",
            "replace EEG Eye State.arff? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctCwkw4TmlMZ",
        "outputId": "474ae1df-c69e-4536-d20e-48d3ae461762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'EEG Eye State.arff'   eeg+eye+state.zip   eeg+eye+state.zip.1\t sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjgpM21kjKNs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io.arff import loadarff\n",
        "raw_data = loadarff('EEG Eye State.arff')\n",
        "df_data = pd.DataFrame(raw_data[0])\n",
        "\n",
        "# Convert stored labels into integers\n",
        "df_data['label'] = df_data['eyeDetection'].apply(lambda b: int.from_bytes(b, 'little') - 48)\n",
        "df_data.index = np.arange(0, len(df_data))\n",
        "df_data.drop('eyeDetection', axis=1, inplace=True)\n",
        "df_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "c5Y8y6NumVQt",
        "outputId": "851d0414-1fe0-4561-cf56-fbef830f7084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           AF3       F7       F3      FC5       T7       P7       O1       O2  \\\n",
              "0      4329.23  4009.23  4289.23  4148.21  4350.26  4586.15  4096.92  4641.03   \n",
              "1      4324.62  4004.62  4293.85  4148.72  4342.05  4586.67  4097.44  4638.97   \n",
              "2      4327.69  4006.67  4295.38  4156.41  4336.92  4583.59  4096.92  4630.26   \n",
              "3      4328.72  4011.79  4296.41  4155.90  4343.59  4582.56  4097.44  4630.77   \n",
              "4      4326.15  4011.79  4292.31  4151.28  4347.69  4586.67  4095.90  4627.69   \n",
              "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
              "14975  4281.03  3990.26  4245.64  4116.92  4333.85  4614.36  4074.87  4625.64   \n",
              "14976  4276.92  3991.79  4245.13  4110.77  4332.82  4615.38  4073.33  4621.54   \n",
              "14977  4277.44  3990.77  4246.67  4113.85  4333.33  4615.38  4072.82  4623.59   \n",
              "14978  4284.62  3991.79  4251.28  4122.05  4334.36  4616.41  4080.51  4628.72   \n",
              "14979  4287.69  3997.44  4260.00  4121.03  4333.33  4616.41  4088.72  4638.46   \n",
              "\n",
              "            P8       T8      FC6       F4       F8      AF4  label  \n",
              "0      4222.05  4238.46  4211.28  4280.51  4635.90  4393.85      0  \n",
              "1      4210.77  4226.67  4207.69  4279.49  4632.82  4384.10      0  \n",
              "2      4207.69  4222.05  4206.67  4282.05  4628.72  4389.23      0  \n",
              "3      4217.44  4235.38  4210.77  4287.69  4632.31  4396.41      0  \n",
              "4      4210.77  4244.10  4212.82  4288.21  4632.82  4398.46      0  \n",
              "...        ...      ...      ...      ...      ...      ...    ...  \n",
              "14975  4203.08  4221.54  4171.28  4269.23  4593.33  4340.51      1  \n",
              "14976  4194.36  4217.44  4162.56  4259.49  4590.26  4333.33      1  \n",
              "14977  4193.33  4212.82  4160.51  4257.95  4591.79  4339.49      1  \n",
              "14978  4200.00  4220.00  4165.64  4267.18  4596.41  4350.77      1  \n",
              "14979  4212.31  4226.67  4167.69  4274.36  4597.95  4350.77      1  \n",
              "\n",
              "[14980 rows x 15 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a4ebc083-69a3-48fb-adcb-f346b41a738b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AF3</th>\n",
              "      <th>F7</th>\n",
              "      <th>F3</th>\n",
              "      <th>FC5</th>\n",
              "      <th>T7</th>\n",
              "      <th>P7</th>\n",
              "      <th>O1</th>\n",
              "      <th>O2</th>\n",
              "      <th>P8</th>\n",
              "      <th>T8</th>\n",
              "      <th>FC6</th>\n",
              "      <th>F4</th>\n",
              "      <th>F8</th>\n",
              "      <th>AF4</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4329.23</td>\n",
              "      <td>4009.23</td>\n",
              "      <td>4289.23</td>\n",
              "      <td>4148.21</td>\n",
              "      <td>4350.26</td>\n",
              "      <td>4586.15</td>\n",
              "      <td>4096.92</td>\n",
              "      <td>4641.03</td>\n",
              "      <td>4222.05</td>\n",
              "      <td>4238.46</td>\n",
              "      <td>4211.28</td>\n",
              "      <td>4280.51</td>\n",
              "      <td>4635.90</td>\n",
              "      <td>4393.85</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4324.62</td>\n",
              "      <td>4004.62</td>\n",
              "      <td>4293.85</td>\n",
              "      <td>4148.72</td>\n",
              "      <td>4342.05</td>\n",
              "      <td>4586.67</td>\n",
              "      <td>4097.44</td>\n",
              "      <td>4638.97</td>\n",
              "      <td>4210.77</td>\n",
              "      <td>4226.67</td>\n",
              "      <td>4207.69</td>\n",
              "      <td>4279.49</td>\n",
              "      <td>4632.82</td>\n",
              "      <td>4384.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4327.69</td>\n",
              "      <td>4006.67</td>\n",
              "      <td>4295.38</td>\n",
              "      <td>4156.41</td>\n",
              "      <td>4336.92</td>\n",
              "      <td>4583.59</td>\n",
              "      <td>4096.92</td>\n",
              "      <td>4630.26</td>\n",
              "      <td>4207.69</td>\n",
              "      <td>4222.05</td>\n",
              "      <td>4206.67</td>\n",
              "      <td>4282.05</td>\n",
              "      <td>4628.72</td>\n",
              "      <td>4389.23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4328.72</td>\n",
              "      <td>4011.79</td>\n",
              "      <td>4296.41</td>\n",
              "      <td>4155.90</td>\n",
              "      <td>4343.59</td>\n",
              "      <td>4582.56</td>\n",
              "      <td>4097.44</td>\n",
              "      <td>4630.77</td>\n",
              "      <td>4217.44</td>\n",
              "      <td>4235.38</td>\n",
              "      <td>4210.77</td>\n",
              "      <td>4287.69</td>\n",
              "      <td>4632.31</td>\n",
              "      <td>4396.41</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4326.15</td>\n",
              "      <td>4011.79</td>\n",
              "      <td>4292.31</td>\n",
              "      <td>4151.28</td>\n",
              "      <td>4347.69</td>\n",
              "      <td>4586.67</td>\n",
              "      <td>4095.90</td>\n",
              "      <td>4627.69</td>\n",
              "      <td>4210.77</td>\n",
              "      <td>4244.10</td>\n",
              "      <td>4212.82</td>\n",
              "      <td>4288.21</td>\n",
              "      <td>4632.82</td>\n",
              "      <td>4398.46</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14975</th>\n",
              "      <td>4281.03</td>\n",
              "      <td>3990.26</td>\n",
              "      <td>4245.64</td>\n",
              "      <td>4116.92</td>\n",
              "      <td>4333.85</td>\n",
              "      <td>4614.36</td>\n",
              "      <td>4074.87</td>\n",
              "      <td>4625.64</td>\n",
              "      <td>4203.08</td>\n",
              "      <td>4221.54</td>\n",
              "      <td>4171.28</td>\n",
              "      <td>4269.23</td>\n",
              "      <td>4593.33</td>\n",
              "      <td>4340.51</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14976</th>\n",
              "      <td>4276.92</td>\n",
              "      <td>3991.79</td>\n",
              "      <td>4245.13</td>\n",
              "      <td>4110.77</td>\n",
              "      <td>4332.82</td>\n",
              "      <td>4615.38</td>\n",
              "      <td>4073.33</td>\n",
              "      <td>4621.54</td>\n",
              "      <td>4194.36</td>\n",
              "      <td>4217.44</td>\n",
              "      <td>4162.56</td>\n",
              "      <td>4259.49</td>\n",
              "      <td>4590.26</td>\n",
              "      <td>4333.33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14977</th>\n",
              "      <td>4277.44</td>\n",
              "      <td>3990.77</td>\n",
              "      <td>4246.67</td>\n",
              "      <td>4113.85</td>\n",
              "      <td>4333.33</td>\n",
              "      <td>4615.38</td>\n",
              "      <td>4072.82</td>\n",
              "      <td>4623.59</td>\n",
              "      <td>4193.33</td>\n",
              "      <td>4212.82</td>\n",
              "      <td>4160.51</td>\n",
              "      <td>4257.95</td>\n",
              "      <td>4591.79</td>\n",
              "      <td>4339.49</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14978</th>\n",
              "      <td>4284.62</td>\n",
              "      <td>3991.79</td>\n",
              "      <td>4251.28</td>\n",
              "      <td>4122.05</td>\n",
              "      <td>4334.36</td>\n",
              "      <td>4616.41</td>\n",
              "      <td>4080.51</td>\n",
              "      <td>4628.72</td>\n",
              "      <td>4200.00</td>\n",
              "      <td>4220.00</td>\n",
              "      <td>4165.64</td>\n",
              "      <td>4267.18</td>\n",
              "      <td>4596.41</td>\n",
              "      <td>4350.77</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14979</th>\n",
              "      <td>4287.69</td>\n",
              "      <td>3997.44</td>\n",
              "      <td>4260.00</td>\n",
              "      <td>4121.03</td>\n",
              "      <td>4333.33</td>\n",
              "      <td>4616.41</td>\n",
              "      <td>4088.72</td>\n",
              "      <td>4638.46</td>\n",
              "      <td>4212.31</td>\n",
              "      <td>4226.67</td>\n",
              "      <td>4167.69</td>\n",
              "      <td>4274.36</td>\n",
              "      <td>4597.95</td>\n",
              "      <td>4350.77</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14980 rows × 15 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4ebc083-69a3-48fb-adcb-f346b41a738b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a4ebc083-69a3-48fb-adcb-f346b41a738b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a4ebc083-69a3-48fb-adcb-f346b41a738b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data['label'].plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "IaIEM4xloiDq",
        "outputId": "81ff8f05-5caa-48d1-f934-45f4b0708bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the target variable detection in the dataset\n",
        "Generate a list of indexes that hold target values"
      ],
      "metadata": {
        "id": "PR4W3ErSM2K8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "def get_label_indexes(df, column_name, relate, value):\n",
        "    return [i for i, x in enumerate(map(relate, df[column_name], [value]*len(df))) if x]"
      ],
      "metadata": {
        "id": "3g7S-woJmivU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define what counts as a label in your data\n",
        "label_indexes = get_label_indexes(df_data, 'label', operator.add, 1)\n",
        "label_indexes[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbe7yRfuoQ07",
        "outputId": "48e4d6c5-cb09-4e68-dd32-04323814274e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create windows of data\n",
        "Can be used directly for models expecting 3D input or can be averaged over specific time periods."
      ],
      "metadata": {
        "id": "EYQ1bfF-u2hH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create fixed windows of data corresponding to the target values\n",
        "Can be overlapping or non-overlapping windows"
      ],
      "metadata": {
        "id": "sPnhkNmhM_jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any\n",
        "\n",
        "def get_fixed_window_information(df: Any, label_indexes: List[int], window_width: int, future_steps: int, label_column: str, overlap: bool = False) -> List[Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Get window information for the given DataFrame based on label_indexes.\n",
        "    Uses fixed window sizes.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame containing the data.\n",
        "        label_indexes: A list of indexes representing the target values.\n",
        "        window_width: The width of the sliding window.\n",
        "        future_steps: The number of steps to look into the future.\n",
        "        label_column: The name of the label column in the DataFrame.\n",
        "        overlap: If set to True, overlapping windows will be allowed (default is False).\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries containing window information with keys 'start_index', 'stop_index', 'label_index', and 'label_value'.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    prev_stop_index = -1\n",
        "\n",
        "    for idx in label_indexes:\n",
        "        stop_index = idx - future_steps\n",
        "        start_index = stop_index - window_width\n",
        "\n",
        "        if not overlap and start_index <= prev_stop_index:\n",
        "            continue\n",
        "\n",
        "        # Ensure the start_index is non-negative and the window size is equal to window_width\n",
        "        if start_index >= 0 and stop_index - start_index == window_width:\n",
        "            window_info = {\n",
        "                'start_index': start_index,\n",
        "                'stop_index': stop_index,\n",
        "                'label_index': idx,\n",
        "                'label_value': df[label_column][idx]\n",
        "            }\n",
        "            data.append(window_info)\n",
        "            prev_stop_index = stop_index\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "95cJI_ZgodBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_info = get_fixed_window_information(df_data, label_indexes, 500, 100, 'label', overlap=False)\n",
        "window_info"
      ],
      "metadata": {
        "id": "78x75INSu4v9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1acf790e-7306-4483-82ec-1b2348ca6e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'start_index': 0, 'stop_index': 500, 'label_index': 600, 'label_value': 1},\n",
              " {'start_index': 501,\n",
              "  'stop_index': 1001,\n",
              "  'label_index': 1101,\n",
              "  'label_value': 0},\n",
              " {'start_index': 1002,\n",
              "  'stop_index': 1502,\n",
              "  'label_index': 1602,\n",
              "  'label_value': 1},\n",
              " {'start_index': 1503,\n",
              "  'stop_index': 2003,\n",
              "  'label_index': 2103,\n",
              "  'label_value': 0},\n",
              " {'start_index': 2004,\n",
              "  'stop_index': 2504,\n",
              "  'label_index': 2604,\n",
              "  'label_value': 1},\n",
              " {'start_index': 2505,\n",
              "  'stop_index': 3005,\n",
              "  'label_index': 3105,\n",
              "  'label_value': 0},\n",
              " {'start_index': 3006,\n",
              "  'stop_index': 3506,\n",
              "  'label_index': 3606,\n",
              "  'label_value': 1},\n",
              " {'start_index': 3507,\n",
              "  'stop_index': 4007,\n",
              "  'label_index': 4107,\n",
              "  'label_value': 1},\n",
              " {'start_index': 4008,\n",
              "  'stop_index': 4508,\n",
              "  'label_index': 4608,\n",
              "  'label_value': 0},\n",
              " {'start_index': 4509,\n",
              "  'stop_index': 5009,\n",
              "  'label_index': 5109,\n",
              "  'label_value': 0},\n",
              " {'start_index': 5010,\n",
              "  'stop_index': 5510,\n",
              "  'label_index': 5610,\n",
              "  'label_value': 1},\n",
              " {'start_index': 5511,\n",
              "  'stop_index': 6011,\n",
              "  'label_index': 6111,\n",
              "  'label_value': 0},\n",
              " {'start_index': 6012,\n",
              "  'stop_index': 6512,\n",
              "  'label_index': 6612,\n",
              "  'label_value': 0},\n",
              " {'start_index': 6513,\n",
              "  'stop_index': 7013,\n",
              "  'label_index': 7113,\n",
              "  'label_value': 1},\n",
              " {'start_index': 7014,\n",
              "  'stop_index': 7514,\n",
              "  'label_index': 7614,\n",
              "  'label_value': 1},\n",
              " {'start_index': 7515,\n",
              "  'stop_index': 8015,\n",
              "  'label_index': 8115,\n",
              "  'label_value': 1},\n",
              " {'start_index': 8016,\n",
              "  'stop_index': 8516,\n",
              "  'label_index': 8616,\n",
              "  'label_value': 1},\n",
              " {'start_index': 8517,\n",
              "  'stop_index': 9017,\n",
              "  'label_index': 9117,\n",
              "  'label_value': 0},\n",
              " {'start_index': 9018,\n",
              "  'stop_index': 9518,\n",
              "  'label_index': 9618,\n",
              "  'label_value': 0},\n",
              " {'start_index': 9519,\n",
              "  'stop_index': 10019,\n",
              "  'label_index': 10119,\n",
              "  'label_value': 0},\n",
              " {'start_index': 10020,\n",
              "  'stop_index': 10520,\n",
              "  'label_index': 10620,\n",
              "  'label_value': 0},\n",
              " {'start_index': 10521,\n",
              "  'stop_index': 11021,\n",
              "  'label_index': 11121,\n",
              "  'label_value': 1},\n",
              " {'start_index': 11022,\n",
              "  'stop_index': 11522,\n",
              "  'label_index': 11622,\n",
              "  'label_value': 1},\n",
              " {'start_index': 11523,\n",
              "  'stop_index': 12023,\n",
              "  'label_index': 12123,\n",
              "  'label_value': 0},\n",
              " {'start_index': 12024,\n",
              "  'stop_index': 12524,\n",
              "  'label_index': 12624,\n",
              "  'label_value': 0},\n",
              " {'start_index': 12525,\n",
              "  'stop_index': 13025,\n",
              "  'label_index': 13125,\n",
              "  'label_value': 0},\n",
              " {'start_index': 13026,\n",
              "  'stop_index': 13526,\n",
              "  'label_index': 13626,\n",
              "  'label_value': 0},\n",
              " {'start_index': 13527,\n",
              "  'stop_index': 14027,\n",
              "  'label_index': 14127,\n",
              "  'label_value': 0},\n",
              " {'start_index': 14028,\n",
              "  'stop_index': 14528,\n",
              "  'label_index': 14628,\n",
              "  'label_value': 0}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "600 in label_indexes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNarqsgR4c4e",
        "outputId": "81e819f5-3140-4a21-9896-3518c3cc243e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ts_windows(window_info, df, label_col):\n",
        "  ts_data_list = []\n",
        "  ts_label_list = []\n",
        "  labels = df[label_col]\n",
        "  df = df.drop(label_col, axis=1)\n",
        "  for window in window_info:\n",
        "    start = window['start_index']\n",
        "    end = window['stop_index']\n",
        "    ts_data_list.append(df.iloc[start:end])\n",
        "    ts_label_list.append(window['label_value'])\n",
        "  return ts_data_list, ts_label_list"
      ],
      "metadata": {
        "id": "RVpF7WU-I3vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_df, y_list = extract_ts_windows(window_info, df_data, 'label')\n",
        "len(X_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duJGYn00JsS1",
        "outputId": "5b45e807-3b4d-4af2-9a02-6adbed1ac80a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_df[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "BWp_GiPmJvuj",
        "outputId": "388a26a4-ead5-41f1-e788-95ac736baccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         AF3       F7       F3      FC5       T7       P7       O1       O2  \\\n",
              "0    4329.23  4009.23  4289.23  4148.21  4350.26  4586.15  4096.92  4641.03   \n",
              "1    4324.62  4004.62  4293.85  4148.72  4342.05  4586.67  4097.44  4638.97   \n",
              "2    4327.69  4006.67  4295.38  4156.41  4336.92  4583.59  4096.92  4630.26   \n",
              "3    4328.72  4011.79  4296.41  4155.90  4343.59  4582.56  4097.44  4630.77   \n",
              "4    4326.15  4011.79  4292.31  4151.28  4347.69  4586.67  4095.90  4627.69   \n",
              "..       ...      ...      ...      ...      ...      ...      ...      ...   \n",
              "495  4290.77  4000.51  4248.72  4092.31  4338.97  4615.38  4090.77  4631.28   \n",
              "496  4288.21  4006.67  4245.13  4091.79  4343.08  4620.00  4098.46  4641.03   \n",
              "497  4284.10  4001.03  4246.15  4084.10  4339.49  4624.10  4106.15  4636.41   \n",
              "498  4278.97  3995.38  4246.15  4084.10  4334.36  4619.49  4103.59  4624.10   \n",
              "499  4264.10  3985.64  4235.38  4078.46  4326.67  4612.82  4101.03  4621.03   \n",
              "\n",
              "          P8       T8      FC6       F4       F8      AF4  \n",
              "0    4222.05  4238.46  4211.28  4280.51  4635.90  4393.85  \n",
              "1    4210.77  4226.67  4207.69  4279.49  4632.82  4384.10  \n",
              "2    4207.69  4222.05  4206.67  4282.05  4628.72  4389.23  \n",
              "3    4217.44  4235.38  4210.77  4287.69  4632.31  4396.41  \n",
              "4    4210.77  4244.10  4212.82  4288.21  4632.82  4398.46  \n",
              "..       ...      ...      ...      ...      ...      ...  \n",
              "495  4229.23  4230.77  4194.87  4284.10  4624.62  4357.95  \n",
              "496  4233.85  4241.54  4197.44  4283.59  4625.64  4350.26  \n",
              "497  4217.44  4233.33  4192.82  4280.51  4621.54  4338.97  \n",
              "498  4197.44  4217.44  4185.13  4276.41  4615.90  4334.36  \n",
              "499  4192.82  4208.21  4175.90  4272.31  4603.59  4327.69  \n",
              "\n",
              "[500 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c1c7846-dcad-492c-8e94-d179165acbf3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AF3</th>\n",
              "      <th>F7</th>\n",
              "      <th>F3</th>\n",
              "      <th>FC5</th>\n",
              "      <th>T7</th>\n",
              "      <th>P7</th>\n",
              "      <th>O1</th>\n",
              "      <th>O2</th>\n",
              "      <th>P8</th>\n",
              "      <th>T8</th>\n",
              "      <th>FC6</th>\n",
              "      <th>F4</th>\n",
              "      <th>F8</th>\n",
              "      <th>AF4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4329.23</td>\n",
              "      <td>4009.23</td>\n",
              "      <td>4289.23</td>\n",
              "      <td>4148.21</td>\n",
              "      <td>4350.26</td>\n",
              "      <td>4586.15</td>\n",
              "      <td>4096.92</td>\n",
              "      <td>4641.03</td>\n",
              "      <td>4222.05</td>\n",
              "      <td>4238.46</td>\n",
              "      <td>4211.28</td>\n",
              "      <td>4280.51</td>\n",
              "      <td>4635.90</td>\n",
              "      <td>4393.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4324.62</td>\n",
              "      <td>4004.62</td>\n",
              "      <td>4293.85</td>\n",
              "      <td>4148.72</td>\n",
              "      <td>4342.05</td>\n",
              "      <td>4586.67</td>\n",
              "      <td>4097.44</td>\n",
              "      <td>4638.97</td>\n",
              "      <td>4210.77</td>\n",
              "      <td>4226.67</td>\n",
              "      <td>4207.69</td>\n",
              "      <td>4279.49</td>\n",
              "      <td>4632.82</td>\n",
              "      <td>4384.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4327.69</td>\n",
              "      <td>4006.67</td>\n",
              "      <td>4295.38</td>\n",
              "      <td>4156.41</td>\n",
              "      <td>4336.92</td>\n",
              "      <td>4583.59</td>\n",
              "      <td>4096.92</td>\n",
              "      <td>4630.26</td>\n",
              "      <td>4207.69</td>\n",
              "      <td>4222.05</td>\n",
              "      <td>4206.67</td>\n",
              "      <td>4282.05</td>\n",
              "      <td>4628.72</td>\n",
              "      <td>4389.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4328.72</td>\n",
              "      <td>4011.79</td>\n",
              "      <td>4296.41</td>\n",
              "      <td>4155.90</td>\n",
              "      <td>4343.59</td>\n",
              "      <td>4582.56</td>\n",
              "      <td>4097.44</td>\n",
              "      <td>4630.77</td>\n",
              "      <td>4217.44</td>\n",
              "      <td>4235.38</td>\n",
              "      <td>4210.77</td>\n",
              "      <td>4287.69</td>\n",
              "      <td>4632.31</td>\n",
              "      <td>4396.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4326.15</td>\n",
              "      <td>4011.79</td>\n",
              "      <td>4292.31</td>\n",
              "      <td>4151.28</td>\n",
              "      <td>4347.69</td>\n",
              "      <td>4586.67</td>\n",
              "      <td>4095.90</td>\n",
              "      <td>4627.69</td>\n",
              "      <td>4210.77</td>\n",
              "      <td>4244.10</td>\n",
              "      <td>4212.82</td>\n",
              "      <td>4288.21</td>\n",
              "      <td>4632.82</td>\n",
              "      <td>4398.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>4290.77</td>\n",
              "      <td>4000.51</td>\n",
              "      <td>4248.72</td>\n",
              "      <td>4092.31</td>\n",
              "      <td>4338.97</td>\n",
              "      <td>4615.38</td>\n",
              "      <td>4090.77</td>\n",
              "      <td>4631.28</td>\n",
              "      <td>4229.23</td>\n",
              "      <td>4230.77</td>\n",
              "      <td>4194.87</td>\n",
              "      <td>4284.10</td>\n",
              "      <td>4624.62</td>\n",
              "      <td>4357.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>4288.21</td>\n",
              "      <td>4006.67</td>\n",
              "      <td>4245.13</td>\n",
              "      <td>4091.79</td>\n",
              "      <td>4343.08</td>\n",
              "      <td>4620.00</td>\n",
              "      <td>4098.46</td>\n",
              "      <td>4641.03</td>\n",
              "      <td>4233.85</td>\n",
              "      <td>4241.54</td>\n",
              "      <td>4197.44</td>\n",
              "      <td>4283.59</td>\n",
              "      <td>4625.64</td>\n",
              "      <td>4350.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>4284.10</td>\n",
              "      <td>4001.03</td>\n",
              "      <td>4246.15</td>\n",
              "      <td>4084.10</td>\n",
              "      <td>4339.49</td>\n",
              "      <td>4624.10</td>\n",
              "      <td>4106.15</td>\n",
              "      <td>4636.41</td>\n",
              "      <td>4217.44</td>\n",
              "      <td>4233.33</td>\n",
              "      <td>4192.82</td>\n",
              "      <td>4280.51</td>\n",
              "      <td>4621.54</td>\n",
              "      <td>4338.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>4278.97</td>\n",
              "      <td>3995.38</td>\n",
              "      <td>4246.15</td>\n",
              "      <td>4084.10</td>\n",
              "      <td>4334.36</td>\n",
              "      <td>4619.49</td>\n",
              "      <td>4103.59</td>\n",
              "      <td>4624.10</td>\n",
              "      <td>4197.44</td>\n",
              "      <td>4217.44</td>\n",
              "      <td>4185.13</td>\n",
              "      <td>4276.41</td>\n",
              "      <td>4615.90</td>\n",
              "      <td>4334.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>4264.10</td>\n",
              "      <td>3985.64</td>\n",
              "      <td>4235.38</td>\n",
              "      <td>4078.46</td>\n",
              "      <td>4326.67</td>\n",
              "      <td>4612.82</td>\n",
              "      <td>4101.03</td>\n",
              "      <td>4621.03</td>\n",
              "      <td>4192.82</td>\n",
              "      <td>4208.21</td>\n",
              "      <td>4175.90</td>\n",
              "      <td>4272.31</td>\n",
              "      <td>4603.59</td>\n",
              "      <td>4327.69</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c1c7846-dcad-492c-8e94-d179165acbf3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6c1c7846-dcad-492c-8e94-d179165acbf3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6c1c7846-dcad-492c-8e94-d179165acbf3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_961tXzKmnY",
        "outputId": "f51ea9cd-b87f-4e8b-efb5-3eea495b4836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dynamic windows of data corresponding to the target values\n",
        "Can be overlapping or non-overlapping windows"
      ],
      "metadata": {
        "id": "8TVTnQvyvMeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "LxFN0o1lvVdd",
        "outputId": "d673d0bc-0699-48a0-cba3-27d809197dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           AF3       F7       F3      FC5       T7       P7       O1       O2  \\\n",
              "0      4329.23  4009.23  4289.23  4148.21  4350.26  4586.15  4096.92  4641.03   \n",
              "1      4324.62  4004.62  4293.85  4148.72  4342.05  4586.67  4097.44  4638.97   \n",
              "2      4327.69  4006.67  4295.38  4156.41  4336.92  4583.59  4096.92  4630.26   \n",
              "3      4328.72  4011.79  4296.41  4155.90  4343.59  4582.56  4097.44  4630.77   \n",
              "4      4326.15  4011.79  4292.31  4151.28  4347.69  4586.67  4095.90  4627.69   \n",
              "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
              "14975  4281.03  3990.26  4245.64  4116.92  4333.85  4614.36  4074.87  4625.64   \n",
              "14976  4276.92  3991.79  4245.13  4110.77  4332.82  4615.38  4073.33  4621.54   \n",
              "14977  4277.44  3990.77  4246.67  4113.85  4333.33  4615.38  4072.82  4623.59   \n",
              "14978  4284.62  3991.79  4251.28  4122.05  4334.36  4616.41  4080.51  4628.72   \n",
              "14979  4287.69  3997.44  4260.00  4121.03  4333.33  4616.41  4088.72  4638.46   \n",
              "\n",
              "            P8       T8      FC6       F4       F8      AF4  label  \n",
              "0      4222.05  4238.46  4211.28  4280.51  4635.90  4393.85      0  \n",
              "1      4210.77  4226.67  4207.69  4279.49  4632.82  4384.10      0  \n",
              "2      4207.69  4222.05  4206.67  4282.05  4628.72  4389.23      0  \n",
              "3      4217.44  4235.38  4210.77  4287.69  4632.31  4396.41      0  \n",
              "4      4210.77  4244.10  4212.82  4288.21  4632.82  4398.46      0  \n",
              "...        ...      ...      ...      ...      ...      ...    ...  \n",
              "14975  4203.08  4221.54  4171.28  4269.23  4593.33  4340.51      1  \n",
              "14976  4194.36  4217.44  4162.56  4259.49  4590.26  4333.33      1  \n",
              "14977  4193.33  4212.82  4160.51  4257.95  4591.79  4339.49      1  \n",
              "14978  4200.00  4220.00  4165.64  4267.18  4596.41  4350.77      1  \n",
              "14979  4212.31  4226.67  4167.69  4274.36  4597.95  4350.77      1  \n",
              "\n",
              "[14980 rows x 15 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-65b77667-abc6-4bdf-9707-6a82b4613df3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AF3</th>\n",
              "      <th>F7</th>\n",
              "      <th>F3</th>\n",
              "      <th>FC5</th>\n",
              "      <th>T7</th>\n",
              "      <th>P7</th>\n",
              "      <th>O1</th>\n",
              "      <th>O2</th>\n",
              "      <th>P8</th>\n",
              "      <th>T8</th>\n",
              "      <th>FC6</th>\n",
              "      <th>F4</th>\n",
              "      <th>F8</th>\n",
              "      <th>AF4</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4329.23</td>\n",
              "      <td>4009.23</td>\n",
              "      <td>4289.23</td>\n",
              "      <td>4148.21</td>\n",
              "      <td>4350.26</td>\n",
              "      <td>4586.15</td>\n",
              "      <td>4096.92</td>\n",
              "      <td>4641.03</td>\n",
              "      <td>4222.05</td>\n",
              "      <td>4238.46</td>\n",
              "      <td>4211.28</td>\n",
              "      <td>4280.51</td>\n",
              "      <td>4635.90</td>\n",
              "      <td>4393.85</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4324.62</td>\n",
              "      <td>4004.62</td>\n",
              "      <td>4293.85</td>\n",
              "      <td>4148.72</td>\n",
              "      <td>4342.05</td>\n",
              "      <td>4586.67</td>\n",
              "      <td>4097.44</td>\n",
              "      <td>4638.97</td>\n",
              "      <td>4210.77</td>\n",
              "      <td>4226.67</td>\n",
              "      <td>4207.69</td>\n",
              "      <td>4279.49</td>\n",
              "      <td>4632.82</td>\n",
              "      <td>4384.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4327.69</td>\n",
              "      <td>4006.67</td>\n",
              "      <td>4295.38</td>\n",
              "      <td>4156.41</td>\n",
              "      <td>4336.92</td>\n",
              "      <td>4583.59</td>\n",
              "      <td>4096.92</td>\n",
              "      <td>4630.26</td>\n",
              "      <td>4207.69</td>\n",
              "      <td>4222.05</td>\n",
              "      <td>4206.67</td>\n",
              "      <td>4282.05</td>\n",
              "      <td>4628.72</td>\n",
              "      <td>4389.23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4328.72</td>\n",
              "      <td>4011.79</td>\n",
              "      <td>4296.41</td>\n",
              "      <td>4155.90</td>\n",
              "      <td>4343.59</td>\n",
              "      <td>4582.56</td>\n",
              "      <td>4097.44</td>\n",
              "      <td>4630.77</td>\n",
              "      <td>4217.44</td>\n",
              "      <td>4235.38</td>\n",
              "      <td>4210.77</td>\n",
              "      <td>4287.69</td>\n",
              "      <td>4632.31</td>\n",
              "      <td>4396.41</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4326.15</td>\n",
              "      <td>4011.79</td>\n",
              "      <td>4292.31</td>\n",
              "      <td>4151.28</td>\n",
              "      <td>4347.69</td>\n",
              "      <td>4586.67</td>\n",
              "      <td>4095.90</td>\n",
              "      <td>4627.69</td>\n",
              "      <td>4210.77</td>\n",
              "      <td>4244.10</td>\n",
              "      <td>4212.82</td>\n",
              "      <td>4288.21</td>\n",
              "      <td>4632.82</td>\n",
              "      <td>4398.46</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14975</th>\n",
              "      <td>4281.03</td>\n",
              "      <td>3990.26</td>\n",
              "      <td>4245.64</td>\n",
              "      <td>4116.92</td>\n",
              "      <td>4333.85</td>\n",
              "      <td>4614.36</td>\n",
              "      <td>4074.87</td>\n",
              "      <td>4625.64</td>\n",
              "      <td>4203.08</td>\n",
              "      <td>4221.54</td>\n",
              "      <td>4171.28</td>\n",
              "      <td>4269.23</td>\n",
              "      <td>4593.33</td>\n",
              "      <td>4340.51</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14976</th>\n",
              "      <td>4276.92</td>\n",
              "      <td>3991.79</td>\n",
              "      <td>4245.13</td>\n",
              "      <td>4110.77</td>\n",
              "      <td>4332.82</td>\n",
              "      <td>4615.38</td>\n",
              "      <td>4073.33</td>\n",
              "      <td>4621.54</td>\n",
              "      <td>4194.36</td>\n",
              "      <td>4217.44</td>\n",
              "      <td>4162.56</td>\n",
              "      <td>4259.49</td>\n",
              "      <td>4590.26</td>\n",
              "      <td>4333.33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14977</th>\n",
              "      <td>4277.44</td>\n",
              "      <td>3990.77</td>\n",
              "      <td>4246.67</td>\n",
              "      <td>4113.85</td>\n",
              "      <td>4333.33</td>\n",
              "      <td>4615.38</td>\n",
              "      <td>4072.82</td>\n",
              "      <td>4623.59</td>\n",
              "      <td>4193.33</td>\n",
              "      <td>4212.82</td>\n",
              "      <td>4160.51</td>\n",
              "      <td>4257.95</td>\n",
              "      <td>4591.79</td>\n",
              "      <td>4339.49</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14978</th>\n",
              "      <td>4284.62</td>\n",
              "      <td>3991.79</td>\n",
              "      <td>4251.28</td>\n",
              "      <td>4122.05</td>\n",
              "      <td>4334.36</td>\n",
              "      <td>4616.41</td>\n",
              "      <td>4080.51</td>\n",
              "      <td>4628.72</td>\n",
              "      <td>4200.00</td>\n",
              "      <td>4220.00</td>\n",
              "      <td>4165.64</td>\n",
              "      <td>4267.18</td>\n",
              "      <td>4596.41</td>\n",
              "      <td>4350.77</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14979</th>\n",
              "      <td>4287.69</td>\n",
              "      <td>3997.44</td>\n",
              "      <td>4260.00</td>\n",
              "      <td>4121.03</td>\n",
              "      <td>4333.33</td>\n",
              "      <td>4616.41</td>\n",
              "      <td>4088.72</td>\n",
              "      <td>4638.46</td>\n",
              "      <td>4212.31</td>\n",
              "      <td>4226.67</td>\n",
              "      <td>4167.69</td>\n",
              "      <td>4274.36</td>\n",
              "      <td>4597.95</td>\n",
              "      <td>4350.77</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14980 rows × 15 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65b77667-abc6-4bdf-9707-6a82b4613df3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-65b77667-abc6-4bdf-9707-6a82b4613df3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-65b77667-abc6-4bdf-9707-6a82b4613df3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_indexes[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGQCBLO71WJT",
        "outputId": "5713a1f2-cedb-4a31-e196-01ddb4ddaf09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilise these windows of data to train a model"
      ],
      "metadata": {
        "id": "zshSQLdoOrxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGB Trees"
      ],
      "metadata": {
        "id": "6DjYpXSuscFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from xgboost.sklearn import XGBClassifier"
      ],
      "metadata": {
        "id": "uP-axyl9MmId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_flat = [df.to_numpy().flatten() for df in X_df]"
      ],
      "metadata": {
        "id": "Hg60PpH3RqZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_flat), len(X_flat[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nni4kknQUijZ",
        "outputId": "bc0bd3e5-d028-4a97-a9ca-848a13c1ad89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29, 7000)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_flat[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm3f_3vzXBfK",
        "outputId": "ebc2b4b6-973f-4e91-a8e5-2a0132718546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4329.23, 4009.23, 4289.23, ..., 4272.31, 4603.59, 4327.69])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pct = 0.60\n",
        "len_train = round(len(X_flat) * train_pct)\n",
        "\n",
        "X_train, X_test = np.split(X_flat, [int(len_train)])\n",
        "y_train, y_test = np.split(y_list, [int(len_train)])\n",
        "\n",
        "# X_train = X_flat[:len_train]\n",
        "# y_train = y_list[:len_train]\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qwt-GMnsPJAi",
        "outputId": "d1e61957-a865-494c-ebf7-d08601d36cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17, 17, 12, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = XGBRegressor(n_jobs=-1)\n",
        "# model = XGBClassifier(n_jobs=-1)\n",
        "\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjuH6P2KRlE8",
        "outputId": "f970d32a-f835-4a94-82ac-38c0fbb61fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             n_estimators=100, n_jobs=-1, num_parallel_tree=None,\n",
              "             predictor=None, random_state=None, ...)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             n_estimators=100, n_jobs=-1, num_parallel_tree=None,\n",
              "             predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             n_estimators=100, n_jobs=-1, num_parallel_tree=None,\n",
              "             predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "predictions = [round(value) for value in y_pred]\n",
        "print(predictions)\n",
        "print(list(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7h7D5X3R9Oc",
        "outputId": "b37f552a-08cd-4545-c8a9-98280a23180b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate predictions\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qnfAhNnSBlW",
        "outputId": "fb8b729a-cd24-459a-8b0e-66b6ac67bd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 58.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Approaches"
      ],
      "metadata": {
        "id": "OnjWSTo_sggE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pct = 0.60\n",
        "len_train = round(len(X_df) * train_pct)\n",
        "\n",
        "X_train, X_test = np.split(X_df, [int(len_train)])\n",
        "y_train, y_test = np.split(y_list, [int(len_train)])\n",
        "\n",
        "# X_train = X_flat[:len_train]\n",
        "# y_train = y_list[:len_train]\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hRvMP62uwSq",
        "outputId": "f26e54ec-4562-4d63-9812-a44e97ea7051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17, 17, 12, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzrOs15qvFcQ",
        "outputId": "c22017a1-a4b5-4038-c398-86eeea802ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MJZt3gff0rw",
        "outputId": "2dcfcac8-b770-40a2-9a64-0735a09a8aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17, 500, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = X_train.shape[2]"
      ],
      "metadata": {
        "id": "Rtq-nLOJ4Jjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def scale_3d_data(X):\n",
        "    num_samples, timesteps, n_features = X.shape\n",
        "    X_scaled = np.zeros((num_samples, timesteps, n_features))\n",
        "    scalers = []\n",
        "\n",
        "    for i in range(n_features):\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled[:, :, i] = scaler.fit_transform(X[:, :, i])\n",
        "        scalers.append(scaler)\n",
        "\n",
        "    return X_scaled, scalers\n",
        "\n",
        "X_train_scaled, scalers = scale_3d_data(X_train)\n",
        "X_test_scaled = np.zeros_like(X_test)\n",
        "\n",
        "for i in range(n_features):\n",
        "    X_test_scaled[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
        "X_train_scaled.shape, X_test_scaled.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aotssMzMeiHk",
        "outputId": "4e7ef61a-6f01-4116-c536-b3bc111eaa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17, 500, 14), (12, 500, 14))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LSTM"
      ],
      "metadata": {
        "id": "zNjiskRC2ut7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Input, LSTM, Dropout, Dense\n",
        "\n",
        "LOOK_BACK = 500\n",
        "FORECAST_RANGE = 100\n",
        "\n",
        "input_layer = Input(shape=(LOOK_BACK, n_features))\n",
        "\n",
        "# Regulariser applied to prevent overfitting and improve generalisation\n",
        "lstm = LSTM(500, activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
        "\n",
        "dropout = Dropout(0.2)(lstm)\n",
        "dense = Dense(1, activation='linear')(dropout)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=dense)\n",
        "# Clipvalue helps prevent exploding gradients\n",
        "model.compile(optimizer=Adam(clipvalue=1.0), loss='mse')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIxnY_jU2wv8",
        "outputId": "01648ff9-800c-4a78-8c9d-240a78f9be82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 500, 14)]         0         \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 500)               1030000   \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 500)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 501       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,030,501\n",
            "Trainable params: 1,030,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_scaled, y_train, epochs=5, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "U5vpcwUKeZJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88faf2df-5db8-4c54-b250-1d0b4c226d48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6940 - val_loss: 0.8553\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.4799 - val_loss: 0.5673\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3838 - val_loss: 0.4389\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.4040 - val_loss: 0.4409\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3353 - val_loss: 0.5040\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdc2be663b0>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_scaled)\n",
        "predictions = y_pred\n",
        "print(predictions.flatten())\n",
        "print([round(x) for x in predictions.flatten()])\n",
        "print(list(y_test))"
      ],
      "metadata": {
        "id": "MoJgYDOKeZ9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd27027b-4360-41d8-9a0e-11f01e4c3abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 601ms/step\n",
            "[8.4649843e-01 3.7822881e-01 2.6903397e-01 2.3748325e+05 1.5003524e+14\n",
            " 2.1735640e+03 4.0160147e+08 3.5970062e-01 3.9116716e-01 1.2000152e+10\n",
            " 7.7965564e-01 4.0379828e-01]\n",
            "[1, 0, 0, 237483, 150035236913152, 2174, 401601472, 0, 0, 12000151552, 1, 0]\n",
            "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate predictions\n",
        "accuracy = accuracy_score(y_test, [round(x) for x in predictions.flatten()])\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "metadata": {
        "id": "3AlXk7r8eenV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f12e74f-67f1-4948-fb34-d44267cfc4d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 41.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LSTM-CNN"
      ],
      "metadata": {
        "id": "Pk860YOp2rn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Input, Conv1D, Flatten, Concatenate, Reshape, LSTM, Dropout, Dense\n",
        "\n",
        "LOOK_BACK = 500\n",
        "FORECAST_RANGE = 100\n",
        "\n",
        "input_layer = Input(shape=(LOOK_BACK, n_features))\n",
        "\n",
        "head_list = []\n",
        "for i in range(n_features):\n",
        "    conv_layer_head = Conv1D(filters=4, kernel_size=7, activation='relu')(input_layer)\n",
        "    conv_layer_head_2 = Conv1D(filters=6, kernel_size=11, activation='relu')(conv_layer_head)\n",
        "    conv_layer_flatten = Flatten()(conv_layer_head_2)\n",
        "    head_list.append(conv_layer_flatten)\n",
        "\n",
        "concat_cnn = Concatenate(axis=1)(head_list)\n",
        "reshape = Reshape((head_list[0].shape[1], n_features))(concat_cnn)\n",
        "\n",
        "# Regulariser applied to prevent overfitting and improve generalisation\n",
        "lstm = LSTM(100, activation='relu', kernel_regularizer=l2(0.01))(reshape)\n",
        "\n",
        "dropout = Dropout(0.2)(lstm)\n",
        "dense = Dense(1, activation='linear')(dropout)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=dense)\n",
        "# Clipvalue helps prevent exploding gradients\n",
        "model.compile(optimizer=Adam(clipvalue=1.0), loss='mse')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4MEKYmwsjUx",
        "outputId": "9b901253-1104-4687-b903-d9be9e7ffdf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 500, 14)]    0           []                               \n",
            "                                                                                                  \n",
            " conv1d_28 (Conv1D)             (None, 494, 4)       396         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_30 (Conv1D)             (None, 494, 4)       396         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_32 (Conv1D)             (None, 494, 4)       396         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_34 (Conv1D)             (None, 494, 4)       396         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_36 (Conv1D)             (None, 494, 4)       396         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_38 (Conv1D)             (None, 494, 4)       396         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_40 (Conv1D)             (None, 494, 4)       396         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_42 (Conv1D)             (None, 494, 4)       396         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_44 (Conv1D)             (None, 494, 4)       396         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_46 (Conv1D)             (None, 494, 4)       396         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_48 (Conv1D)             (None, 494, 4)       396         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_50 (Conv1D)             (None, 494, 4)       396         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_52 (Conv1D)             (None, 494, 4)       396         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_54 (Conv1D)             (None, 494, 4)       396         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_29 (Conv1D)             (None, 484, 6)       270         ['conv1d_28[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_31 (Conv1D)             (None, 484, 6)       270         ['conv1d_30[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_33 (Conv1D)             (None, 484, 6)       270         ['conv1d_32[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_35 (Conv1D)             (None, 484, 6)       270         ['conv1d_34[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_37 (Conv1D)             (None, 484, 6)       270         ['conv1d_36[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_39 (Conv1D)             (None, 484, 6)       270         ['conv1d_38[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_41 (Conv1D)             (None, 484, 6)       270         ['conv1d_40[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_43 (Conv1D)             (None, 484, 6)       270         ['conv1d_42[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_45 (Conv1D)             (None, 484, 6)       270         ['conv1d_44[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_47 (Conv1D)             (None, 484, 6)       270         ['conv1d_46[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_49 (Conv1D)             (None, 484, 6)       270         ['conv1d_48[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_51 (Conv1D)             (None, 484, 6)       270         ['conv1d_50[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_53 (Conv1D)             (None, 484, 6)       270         ['conv1d_52[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_55 (Conv1D)             (None, 484, 6)       270         ['conv1d_54[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_14 (Flatten)           (None, 2904)         0           ['conv1d_29[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_15 (Flatten)           (None, 2904)         0           ['conv1d_31[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_16 (Flatten)           (None, 2904)         0           ['conv1d_33[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_17 (Flatten)           (None, 2904)         0           ['conv1d_35[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_18 (Flatten)           (None, 2904)         0           ['conv1d_37[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_19 (Flatten)           (None, 2904)         0           ['conv1d_39[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_20 (Flatten)           (None, 2904)         0           ['conv1d_41[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_21 (Flatten)           (None, 2904)         0           ['conv1d_43[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_22 (Flatten)           (None, 2904)         0           ['conv1d_45[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_23 (Flatten)           (None, 2904)         0           ['conv1d_47[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_24 (Flatten)           (None, 2904)         0           ['conv1d_49[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_25 (Flatten)           (None, 2904)         0           ['conv1d_51[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_26 (Flatten)           (None, 2904)         0           ['conv1d_53[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_27 (Flatten)           (None, 2904)         0           ['conv1d_55[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 40656)        0           ['flatten_14[0][0]',             \n",
            "                                                                  'flatten_15[0][0]',             \n",
            "                                                                  'flatten_16[0][0]',             \n",
            "                                                                  'flatten_17[0][0]',             \n",
            "                                                                  'flatten_18[0][0]',             \n",
            "                                                                  'flatten_19[0][0]',             \n",
            "                                                                  'flatten_20[0][0]',             \n",
            "                                                                  'flatten_21[0][0]',             \n",
            "                                                                  'flatten_22[0][0]',             \n",
            "                                                                  'flatten_23[0][0]',             \n",
            "                                                                  'flatten_24[0][0]',             \n",
            "                                                                  'flatten_25[0][0]',             \n",
            "                                                                  'flatten_26[0][0]',             \n",
            "                                                                  'flatten_27[0][0]']             \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)            (None, 2904, 14)     0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  (None, 100)          46000       ['reshape_1[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 100)          0           ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 1)            101         ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 55,425\n",
            "Trainable params: 55,425\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_scaled, y_train, epochs=5, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "50jSLVTWex7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d6d869-84c0-468d-c035-48e1d5b4beab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.7804 - val_loss: 1.2570\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.7375 - val_loss: 1.1869\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.6808 - val_loss: 1.1195\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.6274 - val_loss: 1.0503\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.5879 - val_loss: 0.9773\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdc2bd772b0>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_scaled)\n",
        "predictions = y_pred\n",
        "print(predictions.flatten())\n",
        "print([round(x) for x in predictions.flatten()])\n",
        "print(list(y_test))"
      ],
      "metadata": {
        "id": "QeV-JkXBeu6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a274ec-8e8d-4ca3-cda2-72ff8529c569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdc2bd5b370> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 923ms/step\n",
            "[1.5216948e-01 2.9117373e-01 1.8157606e-01 7.9393021e+12 3.0069442e+07\n",
            " 4.0127354e+23 2.0432909e+14 7.5623199e-02 1.1687829e-01 6.2886824e+14\n",
            " 1.1227809e-01 7.0691176e-02]\n",
            "[0, 0, 0, 7939302096896, 30069442, 401273537044878672789504, 204329092907008, 0, 0, 628868239065088, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Evaluate predictions - Fails because of outputs being too far offS\n",
        "# accuracy = accuracy_score(y_test, [round(x) for x in predictions.flatten()])\n",
        "# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "metadata": {
        "id": "o4h5aBbieu6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Encoder only"
      ],
      "metadata": {
        "id": "xNrXvRcE-WyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Custom Transformer Architecture"
      ],
      "metadata": {
        "id": "4mbs37CFdvp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "c4bHCx6Df0yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ],
      "metadata": {
        "id": "Cg8__L4UBBp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "tsaVrybIA8fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "lStwO8XSBF_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "pd6qh4vDGScU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ks8FQJ9pBLL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "jvojSkCeA3mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "      def __init__(self, *, vocab_size, d_model):\n",
        "          super(PositionalEmbedding, self).__init__()\n",
        "          self.d_model = d_model\n",
        "          self.pos_encoding = self.positional_encoding(vocab_size, d_model)\n",
        "\n",
        "      def positional_encoding(self, position, d_model):\n",
        "          angle_rads = self.get_angles(\n",
        "              np.arange(position)[:, np.newaxis],\n",
        "              np.arange(d_model)[np.newaxis, :],\n",
        "              d_model)\n",
        "          angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "          angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "          pos_encoding = angle_rads[np.newaxis, ...]\n",
        "          return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "      def get_angles(self, pos, i, d_model):\n",
        "          angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "          return pos * angle_rates\n",
        "\n",
        "      def call(self, x):\n",
        "          length = tf.shape(x)[1]\n",
        "          x = x * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "          x = x + self.pos_encoding[:, :length, :tf.shape(x)[-1]]\n",
        "          return x\n"
      ],
      "metadata": {
        "id": "z238EEWcAkVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "N_BTdFi-GFgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`"
      ],
      "metadata": {
        "id": "5Ro0H4o3GKrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train Transformer"
      ],
      "metadata": {
        "id": "ikVSpKPNdzfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTransformer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate):\n",
        "        super(CustomTransformer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            num_layers=num_layers,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dff=dff,\n",
        "            vocab_size=input_vocab_size,\n",
        "            dropout_rate=dropout_rate\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        context = self.encoder(x)  # (batch_size, context_len, d_model)\n",
        "        return context\n"
      ],
      "metadata": {
        "id": "V71dcRdnGjXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import GlobalAveragePooling1D\n",
        "\n",
        "LOOK_BACK = 500\n",
        "n_features = 14\n",
        "# Vocab size - here determines the number of positional encoding steps possible\n",
        "input_vocab_size = 4000\n",
        "\n",
        "input_layer = Input(shape=(LOOK_BACK, n_features))\n",
        "\n",
        "# Create the custom transformer model\n",
        "custom_transformer = CustomTransformer(\n",
        "    num_layers=4,\n",
        "    d_model=n_features,\n",
        "    num_heads=4,\n",
        "    dff=512,\n",
        "    input_vocab_size=input_vocab_size,\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "\n",
        "# Apply the custom transformer model to the input\n",
        "transformer_output = custom_transformer(input_layer)\n",
        "\n",
        "# Add a dropout layer\n",
        "dropout = Dropout(0.2)(transformer_output)\n",
        "\n",
        "# Add a global average pooling layer\n",
        "global_avg_pool = GlobalAveragePooling1D()(dropout)\n",
        "\n",
        "# Add a dense layer with one unit and a linear activation function\n",
        "dense = Dense(1, activation='linear')(global_avg_pool)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=input_layer, outputs=dense)\n",
        "model.compile(optimizer=Adam(clipvalue=1.0), loss='mse')\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKt7CXhAGZ3i",
        "outputId": "81b8d7eb-992b-4035-a0eb-eba539337a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 500, 14)]         0         \n",
            "                                                                 \n",
            " custom_transformer (CustomT  (None, 500, 14)          72944     \n",
            " ransformer)                                                     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 500, 14)           0         \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 14)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 15        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 72,959\n",
            "Trainable params: 72,959\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHe8E3lFJF3J",
        "outputId": "432cbff4-a372-4885-ef2b-c8cfbfd8a0d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.6768 - val_loss: 0.6159\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.4341 - val_loss: 0.7975\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.1045 - val_loss: 1.2371\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.1299 - val_loss: 1.3984\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1616 - val_loss: 1.2229\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1236 - val_loss: 0.9113\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0307 - val_loss: 0.6573\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0181 - val_loss: 0.4987\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0429 - val_loss: 0.4293\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0478 - val_loss: 0.4130\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0791 - val_loss: 0.4356\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0454 - val_loss: 0.4776\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0484 - val_loss: 0.5164\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0286 - val_loss: 0.5352\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0246 - val_loss: 0.5306\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0249 - val_loss: 0.5050\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0180 - val_loss: 0.4668\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0105 - val_loss: 0.4290\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0117 - val_loss: 0.3990\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0087 - val_loss: 0.3772\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0116 - val_loss: 0.3715\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0097 - val_loss: 0.3801\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0087 - val_loss: 0.4007\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0110 - val_loss: 0.4360\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0083 - val_loss: 0.4796\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0055 - val_loss: 0.5232\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0061 - val_loss: 0.5643\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0067 - val_loss: 0.5916\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0071 - val_loss: 0.5972\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0076 - val_loss: 0.5848\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0031 - val_loss: 0.5613\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0023 - val_loss: 0.5368\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0036 - val_loss: 0.5194\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0036 - val_loss: 0.5067\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0033 - val_loss: 0.4985\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0042 - val_loss: 0.4969\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0045 - val_loss: 0.5037\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0026 - val_loss: 0.5148\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0021 - val_loss: 0.5212\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0027 - val_loss: 0.5213\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0020 - val_loss: 0.5138\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0025 - val_loss: 0.4970\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0017 - val_loss: 0.4752\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 5.2626e-04 - val_loss: 0.4512\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 4.3327e-04 - val_loss: 0.4319\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0016 - val_loss: 0.4197\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0013 - val_loss: 0.4152\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 5.3296e-04 - val_loss: 0.4136\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0024 - val_loss: 0.4182\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0026 - val_loss: 0.4284\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0021 - val_loss: 0.4395\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0014 - val_loss: 0.4531\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0012 - val_loss: 0.4620\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0013 - val_loss: 0.4655\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0011 - val_loss: 0.4607\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0010 - val_loss: 0.4508\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 8.7118e-04 - val_loss: 0.4427\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 8.5046e-04 - val_loss: 0.4361\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.6925e-04 - val_loss: 0.4334\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 9.9106e-04 - val_loss: 0.4363\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0011 - val_loss: 0.4456\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 6.7803e-04 - val_loss: 0.4563\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0018 - val_loss: 0.4728\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0015 - val_loss: 0.4800\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0010 - val_loss: 0.4820\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 7.7518e-04 - val_loss: 0.4785\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 6.8498e-04 - val_loss: 0.4742\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 9.3162e-04 - val_loss: 0.4679\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 9.1463e-04 - val_loss: 0.4582\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 7.1254e-04 - val_loss: 0.4544\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 5.0139e-04 - val_loss: 0.4554\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0016 - val_loss: 0.4613\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 7.6823e-04 - val_loss: 0.4732\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 5.6224e-04 - val_loss: 0.4783\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 4.5948e-04 - val_loss: 0.4817\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0018 - val_loss: 0.4759\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0013 - val_loss: 0.4675\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 7.2935e-04 - val_loss: 0.4561\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0016 - val_loss: 0.4507\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 9.2969e-04 - val_loss: 0.4508\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.9786e-04 - val_loss: 0.4513\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 6.9944e-04 - val_loss: 0.4506\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 5.8176e-04 - val_loss: 0.4533\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 9.8879e-04 - val_loss: 0.4603\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0012 - val_loss: 0.4631\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 6.0505e-04 - val_loss: 0.4590\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 9.5958e-04 - val_loss: 0.4576\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0011 - val_loss: 0.4503\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 6.8406e-04 - val_loss: 0.4428\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 5.7637e-04 - val_loss: 0.4360\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 6.4332e-04 - val_loss: 0.4345\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 8.8457e-04 - val_loss: 0.4383\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0012 - val_loss: 0.4513\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 4.3113e-04 - val_loss: 0.4672\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 5.7825e-04 - val_loss: 0.4819\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 8.6385e-04 - val_loss: 0.4860\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0011 - val_loss: 0.4802\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 5.3253e-04 - val_loss: 0.4675\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0011 - val_loss: 0.4608\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 5.1753e-04 - val_loss: 0.4570\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdc349bfb50>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_scaled)\n",
        "predictions = y_pred\n",
        "print(predictions.flatten())\n",
        "print([round(x) for x in predictions.flatten()])\n",
        "print(list(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1suZjX-aMbEo",
        "outputId": "2685335a-3093-4347-c9de-628340674449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "[ 0.51068425  0.48225656  0.5615767  -0.15397441 -0.19487841  0.65444374\n",
            " -0.03307545 -0.03261497  0.29188898  0.7195349   0.11552685  0.47769907]\n",
            "[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate predictions\n",
        "accuracy = accuracy_score(y_test, [round(x) for x in predictions.flatten()])\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpI9IxuSd9VK",
        "outputId": "3d3bfbbf-54e4-47ce-d095-5c91a5dda11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 66.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "Think about what to do when we want to use all data since a reading.\n",
        "* Could change window creation to enforce that we use all data between windows with no overlap\n",
        "\n",
        "https://www.phind.com/search?cache=ae7afb8b-5f42-431a-b6d8-0c6dd76f4a5d"
      ],
      "metadata": {
        "id": "uzaZI_1Je0_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_windows(data, y_indices, forecast_range, overlap=False):\n",
        "    windows = []\n",
        "    prev_end = 0\n",
        "    for i in range(len(y_indices) - 1):\n",
        "        start = y_indices[i] if overlap else max(y_indices[i], prev_end)\n",
        "        end = y_indices[i + 1] - forecast_range\n",
        "        window = data[start:end]\n",
        "        windows.append(window)\n",
        "        prev_end = end + 1\n",
        "    return np.array(windows)\n",
        "\n",
        "windows = create_windows(df_data, label_indexes, 100, overlap=False)"
      ],
      "metadata": {
        "id": "GVOZMwDQ0EkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a53844-eeab-4192-9232-0d0ed17a37bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-0bbb6d8f0f2f>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(windows)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def get_window_information(df: Any, label_indexes: List[int], future_steps: int, label_column: str, overlap: bool = False) -> List[Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Get window information for the given DataFrame based on label_indexes.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame containing the data.\n",
        "        label_indexes: A list of indexes representing the target values.\n",
        "        future_steps: The number of steps to look into the future.\n",
        "        label_column: The name of the label column in the DataFrame.\n",
        "        overlap: If set to True, overlapping windows will be allowed (default is False).\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries containing window information with keys 'start_index', 'stop_index', 'label_index', and 'label_value'.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    prev_stop_index = -1\n",
        "\n",
        "    for i in range(len(label_indexes) - 1):\n",
        "        start_index = label_indexes[i]\n",
        "        stop_index = label_indexes[i + 1] - future_steps\n",
        "\n",
        "        if not overlap and stop_index <= prev_stop_index:\n",
        "            continue\n",
        "\n",
        "        # Ensure the start_index is non-negative\n",
        "        if start_index >= 0:\n",
        "            window_info = {\n",
        "                'start_index': start_index,\n",
        "                'stop_index': stop_index,\n",
        "                'label_index': start_index,\n",
        "                'label_value': df[label_column][start_index]\n",
        "            }\n",
        "            data.append(window_info)\n",
        "            prev_stop_index = stop_index\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "windows = get_window_information(df_data, label_indexes, 100, 'label', overlap=False)"
      ],
      "metadata": {
        "id": "UG-tB7GC3a9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_indexes"
      ],
      "metadata": {
        "id": "QmN63XjI3zhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f625f53f-a8c8-4900-fd96-6a7c8b54d930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 25,\n",
              " 26,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 30,\n",
              " 31,\n",
              " 32,\n",
              " 33,\n",
              " 34,\n",
              " 35,\n",
              " 36,\n",
              " 37,\n",
              " 38,\n",
              " 39,\n",
              " 40,\n",
              " 41,\n",
              " 42,\n",
              " 43,\n",
              " 44,\n",
              " 45,\n",
              " 46,\n",
              " 47,\n",
              " 48,\n",
              " 49,\n",
              " 50,\n",
              " 51,\n",
              " 52,\n",
              " 53,\n",
              " 54,\n",
              " 55,\n",
              " 56,\n",
              " 57,\n",
              " 58,\n",
              " 59,\n",
              " 60,\n",
              " 61,\n",
              " 62,\n",
              " 63,\n",
              " 64,\n",
              " 65,\n",
              " 66,\n",
              " 67,\n",
              " 68,\n",
              " 69,\n",
              " 70,\n",
              " 71,\n",
              " 72,\n",
              " 73,\n",
              " 74,\n",
              " 75,\n",
              " 76,\n",
              " 77,\n",
              " 78,\n",
              " 79,\n",
              " 80,\n",
              " 81,\n",
              " 82,\n",
              " 83,\n",
              " 84,\n",
              " 85,\n",
              " 86,\n",
              " 87,\n",
              " 88,\n",
              " 89,\n",
              " 90,\n",
              " 91,\n",
              " 92,\n",
              " 93,\n",
              " 94,\n",
              " 95,\n",
              " 96,\n",
              " 97,\n",
              " 98,\n",
              " 99,\n",
              " 100,\n",
              " 101,\n",
              " 102,\n",
              " 103,\n",
              " 104,\n",
              " 105,\n",
              " 106,\n",
              " 107,\n",
              " 108,\n",
              " 109,\n",
              " 110,\n",
              " 111,\n",
              " 112,\n",
              " 113,\n",
              " 114,\n",
              " 115,\n",
              " 116,\n",
              " 117,\n",
              " 118,\n",
              " 119,\n",
              " 120,\n",
              " 121,\n",
              " 122,\n",
              " 123,\n",
              " 124,\n",
              " 125,\n",
              " 126,\n",
              " 127,\n",
              " 128,\n",
              " 129,\n",
              " 130,\n",
              " 131,\n",
              " 132,\n",
              " 133,\n",
              " 134,\n",
              " 135,\n",
              " 136,\n",
              " 137,\n",
              " 138,\n",
              " 139,\n",
              " 140,\n",
              " 141,\n",
              " 142,\n",
              " 143,\n",
              " 144,\n",
              " 145,\n",
              " 146,\n",
              " 147,\n",
              " 148,\n",
              " 149,\n",
              " 150,\n",
              " 151,\n",
              " 152,\n",
              " 153,\n",
              " 154,\n",
              " 155,\n",
              " 156,\n",
              " 157,\n",
              " 158,\n",
              " 159,\n",
              " 160,\n",
              " 161,\n",
              " 162,\n",
              " 163,\n",
              " 164,\n",
              " 165,\n",
              " 166,\n",
              " 167,\n",
              " 168,\n",
              " 169,\n",
              " 170,\n",
              " 171,\n",
              " 172,\n",
              " 173,\n",
              " 174,\n",
              " 175,\n",
              " 176,\n",
              " 177,\n",
              " 178,\n",
              " 179,\n",
              " 180,\n",
              " 181,\n",
              " 182,\n",
              " 183,\n",
              " 184,\n",
              " 185,\n",
              " 186,\n",
              " 187,\n",
              " 188,\n",
              " 189,\n",
              " 190,\n",
              " 191,\n",
              " 192,\n",
              " 193,\n",
              " 194,\n",
              " 195,\n",
              " 196,\n",
              " 197,\n",
              " 198,\n",
              " 199,\n",
              " 200,\n",
              " 201,\n",
              " 202,\n",
              " 203,\n",
              " 204,\n",
              " 205,\n",
              " 206,\n",
              " 207,\n",
              " 208,\n",
              " 209,\n",
              " 210,\n",
              " 211,\n",
              " 212,\n",
              " 213,\n",
              " 214,\n",
              " 215,\n",
              " 216,\n",
              " 217,\n",
              " 218,\n",
              " 219,\n",
              " 220,\n",
              " 221,\n",
              " 222,\n",
              " 223,\n",
              " 224,\n",
              " 225,\n",
              " 226,\n",
              " 227,\n",
              " 228,\n",
              " 229,\n",
              " 230,\n",
              " 231,\n",
              " 232,\n",
              " 233,\n",
              " 234,\n",
              " 235,\n",
              " 236,\n",
              " 237,\n",
              " 238,\n",
              " 239,\n",
              " 240,\n",
              " 241,\n",
              " 242,\n",
              " 243,\n",
              " 244,\n",
              " 245,\n",
              " 246,\n",
              " 247,\n",
              " 248,\n",
              " 249,\n",
              " 250,\n",
              " 251,\n",
              " 252,\n",
              " 253,\n",
              " 254,\n",
              " 255,\n",
              " 256,\n",
              " 257,\n",
              " 258,\n",
              " 259,\n",
              " 260,\n",
              " 261,\n",
              " 262,\n",
              " 263,\n",
              " 264,\n",
              " 265,\n",
              " 266,\n",
              " 267,\n",
              " 268,\n",
              " 269,\n",
              " 270,\n",
              " 271,\n",
              " 272,\n",
              " 273,\n",
              " 274,\n",
              " 275,\n",
              " 276,\n",
              " 277,\n",
              " 278,\n",
              " 279,\n",
              " 280,\n",
              " 281,\n",
              " 282,\n",
              " 283,\n",
              " 284,\n",
              " 285,\n",
              " 286,\n",
              " 287,\n",
              " 288,\n",
              " 289,\n",
              " 290,\n",
              " 291,\n",
              " 292,\n",
              " 293,\n",
              " 294,\n",
              " 295,\n",
              " 296,\n",
              " 297,\n",
              " 298,\n",
              " 299,\n",
              " 300,\n",
              " 301,\n",
              " 302,\n",
              " 303,\n",
              " 304,\n",
              " 305,\n",
              " 306,\n",
              " 307,\n",
              " 308,\n",
              " 309,\n",
              " 310,\n",
              " 311,\n",
              " 312,\n",
              " 313,\n",
              " 314,\n",
              " 315,\n",
              " 316,\n",
              " 317,\n",
              " 318,\n",
              " 319,\n",
              " 320,\n",
              " 321,\n",
              " 322,\n",
              " 323,\n",
              " 324,\n",
              " 325,\n",
              " 326,\n",
              " 327,\n",
              " 328,\n",
              " 329,\n",
              " 330,\n",
              " 331,\n",
              " 332,\n",
              " 333,\n",
              " 334,\n",
              " 335,\n",
              " 336,\n",
              " 337,\n",
              " 338,\n",
              " 339,\n",
              " 340,\n",
              " 341,\n",
              " 342,\n",
              " 343,\n",
              " 344,\n",
              " 345,\n",
              " 346,\n",
              " 347,\n",
              " 348,\n",
              " 349,\n",
              " 350,\n",
              " 351,\n",
              " 352,\n",
              " 353,\n",
              " 354,\n",
              " 355,\n",
              " 356,\n",
              " 357,\n",
              " 358,\n",
              " 359,\n",
              " 360,\n",
              " 361,\n",
              " 362,\n",
              " 363,\n",
              " 364,\n",
              " 365,\n",
              " 366,\n",
              " 367,\n",
              " 368,\n",
              " 369,\n",
              " 370,\n",
              " 371,\n",
              " 372,\n",
              " 373,\n",
              " 374,\n",
              " 375,\n",
              " 376,\n",
              " 377,\n",
              " 378,\n",
              " 379,\n",
              " 380,\n",
              " 381,\n",
              " 382,\n",
              " 383,\n",
              " 384,\n",
              " 385,\n",
              " 386,\n",
              " 387,\n",
              " 388,\n",
              " 389,\n",
              " 390,\n",
              " 391,\n",
              " 392,\n",
              " 393,\n",
              " 394,\n",
              " 395,\n",
              " 396,\n",
              " 397,\n",
              " 398,\n",
              " 399,\n",
              " 400,\n",
              " 401,\n",
              " 402,\n",
              " 403,\n",
              " 404,\n",
              " 405,\n",
              " 406,\n",
              " 407,\n",
              " 408,\n",
              " 409,\n",
              " 410,\n",
              " 411,\n",
              " 412,\n",
              " 413,\n",
              " 414,\n",
              " 415,\n",
              " 416,\n",
              " 417,\n",
              " 418,\n",
              " 419,\n",
              " 420,\n",
              " 421,\n",
              " 422,\n",
              " 423,\n",
              " 424,\n",
              " 425,\n",
              " 426,\n",
              " 427,\n",
              " 428,\n",
              " 429,\n",
              " 430,\n",
              " 431,\n",
              " 432,\n",
              " 433,\n",
              " 434,\n",
              " 435,\n",
              " 436,\n",
              " 437,\n",
              " 438,\n",
              " 439,\n",
              " 440,\n",
              " 441,\n",
              " 442,\n",
              " 443,\n",
              " 444,\n",
              " 445,\n",
              " 446,\n",
              " 447,\n",
              " 448,\n",
              " 449,\n",
              " 450,\n",
              " 451,\n",
              " 452,\n",
              " 453,\n",
              " 454,\n",
              " 455,\n",
              " 456,\n",
              " 457,\n",
              " 458,\n",
              " 459,\n",
              " 460,\n",
              " 461,\n",
              " 462,\n",
              " 463,\n",
              " 464,\n",
              " 465,\n",
              " 466,\n",
              " 467,\n",
              " 468,\n",
              " 469,\n",
              " 470,\n",
              " 471,\n",
              " 472,\n",
              " 473,\n",
              " 474,\n",
              " 475,\n",
              " 476,\n",
              " 477,\n",
              " 478,\n",
              " 479,\n",
              " 480,\n",
              " 481,\n",
              " 482,\n",
              " 483,\n",
              " 484,\n",
              " 485,\n",
              " 486,\n",
              " 487,\n",
              " 488,\n",
              " 489,\n",
              " 490,\n",
              " 491,\n",
              " 492,\n",
              " 493,\n",
              " 494,\n",
              " 495,\n",
              " 496,\n",
              " 497,\n",
              " 498,\n",
              " 499,\n",
              " 500,\n",
              " 501,\n",
              " 502,\n",
              " 503,\n",
              " 504,\n",
              " 505,\n",
              " 506,\n",
              " 507,\n",
              " 508,\n",
              " 509,\n",
              " 510,\n",
              " 511,\n",
              " 512,\n",
              " 513,\n",
              " 514,\n",
              " 515,\n",
              " 516,\n",
              " 517,\n",
              " 518,\n",
              " 519,\n",
              " 520,\n",
              " 521,\n",
              " 522,\n",
              " 523,\n",
              " 524,\n",
              " 525,\n",
              " 526,\n",
              " 527,\n",
              " 528,\n",
              " 529,\n",
              " 530,\n",
              " 531,\n",
              " 532,\n",
              " 533,\n",
              " 534,\n",
              " 535,\n",
              " 536,\n",
              " 537,\n",
              " 538,\n",
              " 539,\n",
              " 540,\n",
              " 541,\n",
              " 542,\n",
              " 543,\n",
              " 544,\n",
              " 545,\n",
              " 546,\n",
              " 547,\n",
              " 548,\n",
              " 549,\n",
              " 550,\n",
              " 551,\n",
              " 552,\n",
              " 553,\n",
              " 554,\n",
              " 555,\n",
              " 556,\n",
              " 557,\n",
              " 558,\n",
              " 559,\n",
              " 560,\n",
              " 561,\n",
              " 562,\n",
              " 563,\n",
              " 564,\n",
              " 565,\n",
              " 566,\n",
              " 567,\n",
              " 568,\n",
              " 569,\n",
              " 570,\n",
              " 571,\n",
              " 572,\n",
              " 573,\n",
              " 574,\n",
              " 575,\n",
              " 576,\n",
              " 577,\n",
              " 578,\n",
              " 579,\n",
              " 580,\n",
              " 581,\n",
              " 582,\n",
              " 583,\n",
              " 584,\n",
              " 585,\n",
              " 586,\n",
              " 587,\n",
              " 588,\n",
              " 589,\n",
              " 590,\n",
              " 591,\n",
              " 592,\n",
              " 593,\n",
              " 594,\n",
              " 595,\n",
              " 596,\n",
              " 597,\n",
              " 598,\n",
              " 599,\n",
              " 600,\n",
              " 601,\n",
              " 602,\n",
              " 603,\n",
              " 604,\n",
              " 605,\n",
              " 606,\n",
              " 607,\n",
              " 608,\n",
              " 609,\n",
              " 610,\n",
              " 611,\n",
              " 612,\n",
              " 613,\n",
              " 614,\n",
              " 615,\n",
              " 616,\n",
              " 617,\n",
              " 618,\n",
              " 619,\n",
              " 620,\n",
              " 621,\n",
              " 622,\n",
              " 623,\n",
              " 624,\n",
              " 625,\n",
              " 626,\n",
              " 627,\n",
              " 628,\n",
              " 629,\n",
              " 630,\n",
              " 631,\n",
              " 632,\n",
              " 633,\n",
              " 634,\n",
              " 635,\n",
              " 636,\n",
              " 637,\n",
              " 638,\n",
              " 639,\n",
              " 640,\n",
              " 641,\n",
              " 642,\n",
              " 643,\n",
              " 644,\n",
              " 645,\n",
              " 646,\n",
              " 647,\n",
              " 648,\n",
              " 649,\n",
              " 650,\n",
              " 651,\n",
              " 652,\n",
              " 653,\n",
              " 654,\n",
              " 655,\n",
              " 656,\n",
              " 657,\n",
              " 658,\n",
              " 659,\n",
              " 660,\n",
              " 661,\n",
              " 662,\n",
              " 663,\n",
              " 664,\n",
              " 665,\n",
              " 666,\n",
              " 667,\n",
              " 668,\n",
              " 669,\n",
              " 670,\n",
              " 671,\n",
              " 672,\n",
              " 673,\n",
              " 674,\n",
              " 675,\n",
              " 676,\n",
              " 677,\n",
              " 678,\n",
              " 679,\n",
              " 680,\n",
              " 681,\n",
              " 682,\n",
              " 683,\n",
              " 684,\n",
              " 685,\n",
              " 686,\n",
              " 687,\n",
              " 688,\n",
              " 689,\n",
              " 690,\n",
              " 691,\n",
              " 692,\n",
              " 693,\n",
              " 694,\n",
              " 695,\n",
              " 696,\n",
              " 697,\n",
              " 698,\n",
              " 699,\n",
              " 700,\n",
              " 701,\n",
              " 702,\n",
              " 703,\n",
              " 704,\n",
              " 705,\n",
              " 706,\n",
              " 707,\n",
              " 708,\n",
              " 709,\n",
              " 710,\n",
              " 711,\n",
              " 712,\n",
              " 713,\n",
              " 714,\n",
              " 715,\n",
              " 716,\n",
              " 717,\n",
              " 718,\n",
              " 719,\n",
              " 720,\n",
              " 721,\n",
              " 722,\n",
              " 723,\n",
              " 724,\n",
              " 725,\n",
              " 726,\n",
              " 727,\n",
              " 728,\n",
              " 729,\n",
              " 730,\n",
              " 731,\n",
              " 732,\n",
              " 733,\n",
              " 734,\n",
              " 735,\n",
              " 736,\n",
              " 737,\n",
              " 738,\n",
              " 739,\n",
              " 740,\n",
              " 741,\n",
              " 742,\n",
              " 743,\n",
              " 744,\n",
              " 745,\n",
              " 746,\n",
              " 747,\n",
              " 748,\n",
              " 749,\n",
              " 750,\n",
              " 751,\n",
              " 752,\n",
              " 753,\n",
              " 754,\n",
              " 755,\n",
              " 756,\n",
              " 757,\n",
              " 758,\n",
              " 759,\n",
              " 760,\n",
              " 761,\n",
              " 762,\n",
              " 763,\n",
              " 764,\n",
              " 765,\n",
              " 766,\n",
              " 767,\n",
              " 768,\n",
              " 769,\n",
              " 770,\n",
              " 771,\n",
              " 772,\n",
              " 773,\n",
              " 774,\n",
              " 775,\n",
              " 776,\n",
              " 777,\n",
              " 778,\n",
              " 779,\n",
              " 780,\n",
              " 781,\n",
              " 782,\n",
              " 783,\n",
              " 784,\n",
              " 785,\n",
              " 786,\n",
              " 787,\n",
              " 788,\n",
              " 789,\n",
              " 790,\n",
              " 791,\n",
              " 792,\n",
              " 793,\n",
              " 794,\n",
              " 795,\n",
              " 796,\n",
              " 797,\n",
              " 798,\n",
              " 799,\n",
              " 800,\n",
              " 801,\n",
              " 802,\n",
              " 803,\n",
              " 804,\n",
              " 805,\n",
              " 806,\n",
              " 807,\n",
              " 808,\n",
              " 809,\n",
              " 810,\n",
              " 811,\n",
              " 812,\n",
              " 813,\n",
              " 814,\n",
              " 815,\n",
              " 816,\n",
              " 817,\n",
              " 818,\n",
              " 819,\n",
              " 820,\n",
              " 821,\n",
              " 822,\n",
              " 823,\n",
              " 824,\n",
              " 825,\n",
              " 826,\n",
              " 827,\n",
              " 828,\n",
              " 829,\n",
              " 830,\n",
              " 831,\n",
              " 832,\n",
              " 833,\n",
              " 834,\n",
              " 835,\n",
              " 836,\n",
              " 837,\n",
              " 838,\n",
              " 839,\n",
              " 840,\n",
              " 841,\n",
              " 842,\n",
              " 843,\n",
              " 844,\n",
              " 845,\n",
              " 846,\n",
              " 847,\n",
              " 848,\n",
              " 849,\n",
              " 850,\n",
              " 851,\n",
              " 852,\n",
              " 853,\n",
              " 854,\n",
              " 855,\n",
              " 856,\n",
              " 857,\n",
              " 858,\n",
              " 859,\n",
              " 860,\n",
              " 861,\n",
              " 862,\n",
              " 863,\n",
              " 864,\n",
              " 865,\n",
              " 866,\n",
              " 867,\n",
              " 868,\n",
              " 869,\n",
              " 870,\n",
              " 871,\n",
              " 872,\n",
              " 873,\n",
              " 874,\n",
              " 875,\n",
              " 876,\n",
              " 877,\n",
              " 878,\n",
              " 879,\n",
              " 880,\n",
              " 881,\n",
              " 882,\n",
              " 883,\n",
              " 884,\n",
              " 885,\n",
              " 886,\n",
              " 887,\n",
              " 888,\n",
              " 889,\n",
              " 890,\n",
              " 891,\n",
              " 892,\n",
              " 893,\n",
              " 894,\n",
              " 895,\n",
              " 896,\n",
              " 897,\n",
              " 898,\n",
              " 899,\n",
              " 900,\n",
              " 901,\n",
              " 902,\n",
              " 903,\n",
              " 904,\n",
              " 905,\n",
              " 906,\n",
              " 907,\n",
              " 908,\n",
              " 909,\n",
              " 910,\n",
              " 911,\n",
              " 912,\n",
              " 913,\n",
              " 914,\n",
              " 915,\n",
              " 916,\n",
              " 917,\n",
              " 918,\n",
              " 919,\n",
              " 920,\n",
              " 921,\n",
              " 922,\n",
              " 923,\n",
              " 924,\n",
              " 925,\n",
              " 926,\n",
              " 927,\n",
              " 928,\n",
              " 929,\n",
              " 930,\n",
              " 931,\n",
              " 932,\n",
              " 933,\n",
              " 934,\n",
              " 935,\n",
              " 936,\n",
              " 937,\n",
              " 938,\n",
              " 939,\n",
              " 940,\n",
              " 941,\n",
              " 942,\n",
              " 943,\n",
              " 944,\n",
              " 945,\n",
              " 946,\n",
              " 947,\n",
              " 948,\n",
              " 949,\n",
              " 950,\n",
              " 951,\n",
              " 952,\n",
              " 953,\n",
              " 954,\n",
              " 955,\n",
              " 956,\n",
              " 957,\n",
              " 958,\n",
              " 959,\n",
              " 960,\n",
              " 961,\n",
              " 962,\n",
              " 963,\n",
              " 964,\n",
              " 965,\n",
              " 966,\n",
              " 967,\n",
              " 968,\n",
              " 969,\n",
              " 970,\n",
              " 971,\n",
              " 972,\n",
              " 973,\n",
              " 974,\n",
              " 975,\n",
              " 976,\n",
              " 977,\n",
              " 978,\n",
              " 979,\n",
              " 980,\n",
              " 981,\n",
              " 982,\n",
              " 983,\n",
              " 984,\n",
              " 985,\n",
              " 986,\n",
              " 987,\n",
              " 988,\n",
              " 989,\n",
              " 990,\n",
              " 991,\n",
              " 992,\n",
              " 993,\n",
              " 994,\n",
              " 995,\n",
              " 996,\n",
              " 997,\n",
              " 998,\n",
              " 999,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windows[0]"
      ],
      "metadata": {
        "id": "GcmJT0ob0l-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef60a123-c4b8-4faf-d764-6c86705c005c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'start_index': 99, 'stop_index': 0, 'label_index': 99, 'label_value': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windows[1]"
      ],
      "metadata": {
        "id": "lx4vSaI606wM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af71a922-1abb-4ccc-ac56-870617ad5f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'start_index': 100, 'stop_index': 1, 'label_index': 100, 'label_value': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def pad_windows(windows, max_length):\n",
        "    padded_windows = []\n",
        "    for window in windows:\n",
        "        padding = max_length - window.shape[0]\n",
        "        pad = np.zeros((padding, window.shape[1]))\n",
        "        padded_window = np.vstack((window, pad))\n",
        "        padded_windows.append(padded_window)\n",
        "    return np.array(padded_windows)\n",
        "\n",
        "max_length = max([window.shape[0] for window in windows])\n",
        "windows_padded = pad_windows(windows, max_length)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "windows_padded_scaled = np.zeros(windows_padded.shape)\n",
        "\n",
        "for i in range(windows_padded.shape[0]):\n",
        "    windows_padded_scaled[i] = scaler.fit_transform(windows_padded[i])\n"
      ],
      "metadata": {
        "id": "oA15TM2jfLd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "d89ae149-af76-4f29-fb65-e1dbb29999e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-f4ae96dce30d>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_windows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mwindows_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-f4ae96dce30d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_windows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mwindows_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
          ]
        }
      ]
    }
  ]
}